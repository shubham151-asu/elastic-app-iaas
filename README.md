# elastic-app-iaaS

Our cloud app will provide an image recognition service to users, by using cloud resources to perform deep learning on images provided by the users. Deep learning is done by the Pytorch tool to classify CIFAR datasets images using a pre-trained model. The workload for us is to run image classification tasks on a set of images uploaded by the client. The image classification program can run on a single system and give classification results quickly, however as the image count increases to 100 or 1000s, a single system would take a high amount of time to process each of the images. Furthermore, adding a few more similar systems can help solve the issue but it wouldnâ€™t be cost effective to leave the additional systems running when the workload can be handled by a single system. To solve this problem, we have built an elastic application that can automatically scale out and on demand and could save cost by using IaaS cloud services offering from AWS. AWS is the most widely used IaaS provider and offers a variety of compute, storage, and message services. We use some of these services, as part of our overall design. 

We are using AWS EC2 instances to run the classification module. These instances can be manually added as well as removed depending upon the workload by using AWS launch console. Further, AWS also provides a robust automatic scaling solution which uses both dynamic as well as predictive strategy to scale-in and scale-out on-demand. We could have used this scaling strategy for our design, however our approach to auto-scaling is different from AWS. There are three possible explanations for this. First, we understand the behavior of our application like average run-time for the certain workload. Second, we use the queue size as a metric to decide the incoming future workload on our server instances. Third, we know of the incoming high workload even before the instance processes them, we could start other instances to effectively distribute the load before the current instances degrade the latency. This could also save some of the cold start time for the instance and reduces latency in the overall system. Using the understanding of all the three situations, we have come up with a scaling strategy and we believe our scaling strategy could be more efficient for our workload.

# Design and implementation
Our design involves a decoupled architecture consisting of a Web-tier (EC2 instance), controller, and App-tier (EC2 instances) which are glued together by the Amazon SQS message queues while the storage is S3 buckets. Our controller is similar to Amazon Automatic process control, which would create and terminate the instances on-demand. For simplicity, we have put up the controller module into the web-tier instance. Our App-tier instance runs the image classification tasks when a client uploads the image. For our use case, we have one web-tier instance while the app-tier instance can scale-in and scale-out on-demand by the controller. We have created AMIs for the web-tier and app-tier instances to resolve any dependencies issues for the underlying libraries and programs. Due to AMIs, we have the ability to create a one-click solution to deploy our entire application in the cloud and clean up the entire cloud resources if the shut down is required.

![Architecture](https://github.com/shubham151-asu/elastic-app-iaas/blob/main/Architecture.jpg?raw=true)

Our implementation starts with a start phase which runs a JAVA code in our local system that starts the deployment process. The deployment process creates one web-tier instance with the controller module in it. The controller is started with the web-tier instance and it further starts the  app-tier instance, creates SQS request as well as response queue, and also the S3 input and output buckets in the AWS.  We can optionally use the stop script to kill all the running instances and cleans up all resources created during the start phase.

# WorkFlow

When a client uploads image(s) using the Angular application, the backend NodeJS server receives the image and uploads the images to the S3 input bucket and also pushes message(s) (a description of fileName uploaded for classification) into the SQS request queue. The App-tier instances pull a single message from the request queue and parses the message. After parsing, it figures out which file to download from S3 input bucket for the image classification tasks. After the file is downloaded locally on the EC2 instance, the image classification module is called by the App-tier program. Once the classification is done, it uploads the results into the output S3 bucket and sends a message to the SQS response queue to help the NodeJS server know about the classification task completion. The NodeJS server pulls the message(s) from the response queue and thereby returns the output clients about the processed results which were polled by the Angular application. In case of high incoming requests, the request message queue has a high number of messages, the controller module continuously polls the request message queue and based on the size of the queue , it uses an algorithm to decide how many instances to create to meet the incoming requests. When the processing has happened the request message queue would not have as many messages, and hence some of the instances will be terminated.

# Auto-scaling 

We have designed and implemented a controller that scales up the application tier instances up and down based on the number of requests received. In order to gauge the approximate number of pending requests yet to be served, we look at the number of messages present in the request queue. The number of application tier instances required is based not only on the length of the request message queue but also on how long the application tier takes to process each request and the acceptable amount of response latency for each user request.
From these three parameters we calculate two important metrics using which we make the decision to scale up or scale down. These metrics are described below.
Backlog per instance: The backlog per instance indicates the number of requests that would be served by each instance if all the messages in the request queue is divided equally by the number of application tier instances running at that particular time.
Acceptable backlog per instance: The acceptable backlog per instance is calculated by dividing the acceptable response latency with the average time it takes for an application instance to process each request.
For example in our image processing application, we choose the acceptable latency per request to be 2 seconds. We also calculated the average processing time for each image for the t2 micro-instance and found it to be approximately 0.88 seconds. Now, let's say the approximate number of messages in the request queue is 100 and the number of running app tier instances is just one at the beginning. So the backlog per instance in this case becomes equal to 113. However, the acceptable backlog instance is just around 2. In this case, the controller makes the decision to scale up the app by another 49 instances in order to try and reach the acceptable backlog per instance. However according to the project specifications, we can only have 20 running instances at any point of time. Due to this, the controller only spins up 18 new instances in order to scale out.
In the case of scaling down, the controller uses the same logic to find the desirable number of instances. For example, if there are only 20 messages left in the queue and there are 19 application tier instances running. The controller finds that the backlog per instance is just 1 while the acceptable backlog per instance is 2. So the controller decides to scale down by 9 instances so that only 10 application tier instances will run. The controller makes a decision to scale up or down every 6 seconds. However, the controller polls the SQS service and records the number of messages in the queue every two seconds. At the end of every 6 seconds, the average of all the polled data points are calculated and a decision is made. Here, the average of the 3 data points are calculated to smoothen out the inconsistencies in the number of approximate messages returned by the SQS service.
We decided to place the controller inside the web instance so that we could use the extra instance to scale the app tier upto 19 instances. The controller instance was implemented as a Java program. The controller program and all its dependencies are packaged into a single jar file and placed inside the web instance. The controller is launched by a command in the user data script that is executed when the EC2 instance is booted up.

